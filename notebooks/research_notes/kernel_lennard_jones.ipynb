{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4e8f98cf",
   "metadata": {},
   "source": [
    "---\n",
    "title: Kernel learning of a Lennard-Jones potential\n",
    "author: Tristan Bereau\n",
    "date: 'Mar 28, 2023'\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeabef5",
   "metadata": {},
   "source": [
    "Let's learn a Lennard-Jones potential (shown in @fig-lj-plot). We'll use a Gaussian process regression aka kernel. Later we will have a look at the effect of a physically motivated prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c199d-c8f6-4dc3-920c-c5fb590fbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f93b6-76bd-4a10-8ae2-d7a2253f8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lj(r):\n",
    "    return 4. * ((1./r)**12 - (1./r)**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d693ad4-f310-4072-a146-3bf83438aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-lj-plot\n",
    "#| fig-cap: \"Lennard-Jones potential as a function of distance\"\n",
    "\n",
    "distances = np.linspace(0.9, 2.0, 50)\n",
    "\n",
    "X_train, X_test = train_test_split(distances, train_size=0.5, shuffle=True)\n",
    "y_train, y_test = lj(X_train), lj(X_test)\n",
    "X_train = X_train.reshape((len(X_train), 1))\n",
    "X_test = X_test.reshape((len(X_test), 1))\n",
    "\n",
    "\n",
    "plt.plot(X_train, lj(X_train), 'or', label='Training')\n",
    "plt.plot(X_test, lj(X_test), 'og', label='Test')\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfa0ab-1654-4025-9c93-eb60ac40d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GaussianKernel:\n",
    "    sigma: float\n",
    "    regularization: float\n",
    "    alpha: np.array = field(init=False)\n",
    "    training_set: np.array = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.alpha = None\n",
    "        self.training_set = None\n",
    "\n",
    "    @property\n",
    "    def norm(self) -> Literal[\"euclidean\", \"cityblock\"]:\n",
    "        return \"cityblock\"\n",
    "\n",
    "    def get_kernel_matrix(self, x_1: np.array, x_2: np.array) -> np.array:\n",
    "        return np.exp(- cdist(x_1, x_2, self.norm) / self.sigma)\n",
    "\n",
    "    def fit(self, x: np.array, y: np.array) -> None:\n",
    "        self.training_set = x\n",
    "        regularized_kernel = (\n",
    "                self.get_kernel_matrix(x, x)\n",
    "                + self.regularization * np.identity(len(x))\n",
    "        )\n",
    "        self.alpha = np.dot(y, np.linalg.inv(regularized_kernel))\n",
    "\n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        assert self.alpha is not None, \"Model has not been trained yet!\"\n",
    "        return np.dot(self.alpha, self.get_kernel_matrix(self.training_set, x))\n",
    "\n",
    "    def mean_absolute_error(self, x: np.array, y: np.array) -> float:\n",
    "        return mean_absolute_error(y, self.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = GaussianKernel(sigma=0.1, regularization=1e-9)\n",
    "gpr.fit(X_train, y_train)\n",
    "y_predict = gpr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37195668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, y_test, 'or', label='Reference')\n",
    "plt.plot(X_test, y_predict, 'og', label='Predictions')\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d14cf8",
   "metadata": {},
   "source": [
    "The predictions do well overall, except in the repulsive region, where they severely underestimate. We'll improve upon this later by adding some prior information.\n",
    "\n",
    "For now, let's compute a learning curve to better assess the quality of the model. We'll do this by averaging several ML models for a number of training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bffc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr.mean_absolute_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e709299",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LearningCurve:\n",
    "    gpr: GaussianKernel\n",
    "    input_x: np.array\n",
    "    target_y: np.array\n",
    "    number_samples: int = 100\n",
    "\n",
    "    def score(self, training_ratio: float) -> pd.Series:\n",
    "        maes = []\n",
    "        for i in range(self.number_samples):\n",
    "            x_train_, x_test_ = train_test_split(\n",
    "                distances, train_size=training_ratio, shuffle=True\n",
    "            )\n",
    "            y_train_, y_test_ = lj(x_train_), lj(x_test_)\n",
    "            x_train_ = x_train_.reshape((len(x_train_), 1))\n",
    "            x_test_ = x_test_.reshape((len(x_test_), 1))\n",
    "            self.gpr.fit(x_train_, y_train_)\n",
    "            maes.append(self.gpr.mean_absolute_error(x_test_, y_test_))\n",
    "        return pd.Series(data=maes, name=f\"{training_ratio}\")\n",
    "\n",
    "    def scores(self, training_ratios: list[float]) -> pd.DataFrame:\n",
    "        return pd.concat([self.score(ratio) for ratio in training_ratios], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve = LearningCurve(gpr, distances, lj(distances), number_samples=500)\n",
    "df_scores = learning_curve.scores(\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    x=df_scores.columns.values,\n",
    "    y=df_scores.T[\"mean\"].values,\n",
    "    yerr=df_scores.T[\"std\"].to_numpy() / np.sqrt(df_scores.T[\"count\"].to_numpy())\n",
    ")\n",
    "plt.loglog()\n",
    "plt.grid();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abdef5",
   "metadata": {},
   "source": [
    "## Adding a prior to the potential\n",
    "\n",
    "Predictions at short range tend to fail badly, due to difficulties in efficiently learning the divergence. To alleviate this, let's implement a simple linear repulsive prior of the form\n",
    "$$\n",
    " U_{\\rm prior} = -\\gamma(r-r_0)\\theta(r_0-r),\n",
    "$$\n",
    "where $\\gamma$ and $r_0$ are new hyperparameters, which control the slope and range of the function, respectively. $\\theta$ denotes the Heaviside step function (returns 0 if the argument is negative, 1 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GaussianKernelWithPrior(GaussianKernel):\n",
    "    gamma: float\n",
    "    r_0: float\n",
    "\n",
    "    def prior(self, r):\n",
    "        return -self.gamma * (r - self.r_0) * np.heaviside(self.r_0 - r, 0.)\n",
    "\n",
    "    def fit(self, x: np.array, y: np.array) -> None:\n",
    "        self.training_set = x\n",
    "        regularized_kernel = (\n",
    "                self.get_kernel_matrix(x, x)\n",
    "                + self.regularization * np.identity(len(x))\n",
    "        )\n",
    "        y_shifted = y - self.prior(x.T[0])\n",
    "        self.alpha = np.dot(y_shifted, np.linalg.inv(regularized_kernel))\n",
    "\n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        assert self.alpha is not None, \"Model has not been trained yet!\"\n",
    "        return (\n",
    "            self.prior(x.T[0])\n",
    "            + np.dot(self.alpha, self.get_kernel_matrix(self.training_set, x))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_w_prior = GaussianKernelWithPrior(\n",
    "    sigma=0.1, regularization=1e-9, gamma=150., r_0=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_w_prior.fit(X_train, y_train)\n",
    "gpr_w_prior.mean_absolute_error(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_w_prior = LearningCurve(\n",
    "    gpr_w_prior, distances, lj(distances), number_samples=500\n",
    ")\n",
    "df_scores_w_prior = (\n",
    "    learning_curve_w_prior\n",
    "    .scores([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95])\n",
    "    .describe()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    x=df_scores.columns.values,\n",
    "    y=df_scores.T[\"mean\"].values,\n",
    "    yerr=(\n",
    "        df_scores.T[\"std\"].to_numpy()\n",
    "        / np.sqrt(df_scores.T[\"count\"].to_numpy())\n",
    "    ),\n",
    "    label=\"without\",\n",
    ")\n",
    "plt.errorbar(\n",
    "    x=df_scores_w_prior.columns.values,\n",
    "    y=df_scores_w_prior.T[\"mean\"].values,\n",
    "    yerr=(\n",
    "        df_scores_w_prior.T[\"std\"].to_numpy()\n",
    "        / np.sqrt(df_scores_w_prior.T[\"count\"].to_numpy())\n",
    "    ),\n",
    "    label=\"with prior\",\n",
    ")\n",
    "plt.loglog()\n",
    "plt.legend()\n",
    "plt.grid();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dc736",
   "metadata": {},
   "source": [
    "Learning with this simple repulsive prior leads to improved learning performance: while the slope (i.e., learning rate) is roughly the same, the offset is significantly reduced. Note that this is a log-log plot.\n",
    "\n",
    "Statistical information theory predicts that ML models often learn with a power-law behavior between test error and training-set size\n",
    "$$\n",
    "E \\sim \\beta N^\\nu\n",
    "$$\n",
    "where the coefficients $\\beta$ and $\\nu$ dictate the offset and slope of learning, respectively. See papers from Vapnik et al. for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb966c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_scientific_profile",
   "language": "python",
   "name": "my_scientific_profile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
